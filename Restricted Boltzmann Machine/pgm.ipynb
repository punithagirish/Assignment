{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzman Machine \n",
    "\n",
    "A restricted Boltzmann Machine is an \"Energy Based\" generative stochastic model. They were initially invented by Paul Smolensky in 1986 and were called \"Harmonium\". After the evolution of training algorithms in the mid 2000's by Geoffrey Hinton, the boltzman machine became more prominent. They gained big popularity in recent years in the context of the Netflix Prize where RBMs achieved state of the art performance in collaborative filtering and have beaten most of the competition.\n",
    "\n",
    "RBM's are useful for dimensionality reduction, classification, regression, collaborative filtering, feature learning and topic modeling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "RBMs are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input, layer, and the second is the hidden layer. The absense of an output layer is apparent. As we move forward we would learn why the output layer won't be needed.\n",
    "<img src=\"figure1.png\" width=\"150\" height=\"50\" title=\"Layers in RBM\">\n",
    "Figure1: Layers in RBM\n",
    "Each circle in the figure above represents a neuron-like unit called a node, and nodes are simply where calculations take place. \n",
    "<img src=\"figure3.png\" width=\"500\" height=\"200\" title=\"Layers in RBM\">\n",
    "Figure2: Structure of RBM\n",
    "The nodes are connected to each other across layers, but no two nodes of the same layer are linked. That is, there is no intra-layer communication – this is the restriction in a restricted Boltzmann machine. Each node is a locus of computation that processes input, and begins by making stochastic decisions about whether to transmit that input or not. Each visible node takes a low-level feature from an item in the dataset to be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the packages that will be required in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np_rng = np.random.RandomState(1234) #setting the random state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graded\n",
    "# import data\n",
    "\n",
    "df = pd.read_excel(\"amazon.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  So there is no way for me to plug it in here i...\n",
       "1                        Good case, Excellent value.\n",
       "2                             Great for the jawbone.\n",
       "3  Tied to charger for conversations lasting more...\n",
       "4                                  The mic is great."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this and check if you have got the correct output\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Topic Modelling, you find the best set of topics that describe the document. There are various ways to perform topic modelling one of which is RBM. You train your RBM on a set of documents. \n",
    "The visible layers will be the words in the text, the hidden layers will give the Topics. \n",
    "To input words into the visible layer, let's convert the train and test data you split above into a bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "# create bag of words model on train and test data\n",
    "tf = CountVectorizer(input='content', lowercase=True, token_pattern=r\"(?u)\\b\\w\\w+\\b\", analyzer='word',\n",
    "                        max_df=50, min_df=1, binary=False, dtype=np.int64)#the final shape should be (number of documents, vocabulary)\n",
    "\n",
    "# fit tf on the dataframe df\n",
    "tf = tf.fit(df['Text'])\n",
    "\n",
    "# transform df dataframe\n",
    "trainX = tf.transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1825)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if you are getting the correct output\n",
    "print(sum(trainX.toarray()[1]))\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "<br>\n",
    "\n",
    "3 <br>\n",
    "(1000, 1825)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the bag of words model, let's define the number of visible and hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "# define visible units\n",
    "visibleUnits = trainX.shape[1]# vocabulary size ~1 line\n",
    "\n",
    "# assign number of units\n",
    "hiddenUnits = 5 # hyperparameter, this means that we are looking for 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1825"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visibleUnits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "# utility Functions\n",
    "\n",
    "# deine the sigmoid function\n",
    "def sigmoid(X):\n",
    "    return 1.0/(1.0 + np.exp(-1.0*X))# ~ 1 line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM as a Probabilistic Model\n",
    "Restricted Boltzmann Machines are probabilistic. As opposed to assigning discrete values the model assigns probabilities. At each point in time the RBM is in a certain state. The state refers to the values of neurons in the visible and hidden layers v and h. The probability that a certain state of v and h can be observed is given by the following joint distribution:\n",
    "<img src=\"figure4.png\" width=\"200\" height=\"70\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 2. Joint Distribution for v and h.\n",
    "Here Z is called the ‘partition function’ that is the summation over all possible pairs of visible and hidden vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution is known as the Boltzmann Distribution which gives the probability that a particle can be observed in the state with the energy E. Unfortunately it is very difficult to calculate the joint probability due to the huge number of possible combination of v and h in the partition function Z. Much easier is the calculation of the conditional probabilities of state h given the state v and conditional probabilities of state v given the state h:\n",
    "<img src=\"figure5.png\" width=\"200\" height=\"70\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 3. Conditional probabilities for h and v.\n",
    "It should be noticed beforehand (before demonstrating this fact on practical example) that each neuron in a RBM can only exist in a binary state of 0 or 1. The most interesting factor is the probability that a hidden or visible layer neuron is in the state 1 — hence activated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Divergence\n",
    "\n",
    "### Gibbs Sampling\n",
    "The first part of the training is called Gibbs Sampling. Given an input vector v we are using p(h|v) for prediction of the hidden values h via sampling. Knowing the hidden values we use p(v|h) for prediction of new input values v via sampling. This process is repeated k times. After k iterations, we obtain the visible vector $v_k$ which was recreated from original input values $v_0$.\n",
    "<img src=\"figure8.png\" width=\"500\" height=\"300\" title=\"Layers in RBM\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gibbs function *gibbs* is divided into subparts: <br>\n",
    "1.*sampleHiddenLayer * <br>\n",
    "2.*sampleVisibleLayer*\n",
    "\n",
    "Let's look at *sampleHiddenLayer* now.\n",
    "\n",
    "### Sample Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know that given an input vector v the probability for a single hidden neuron j being activated is:\n",
    "<img src=\"figure6.png\" width=\"400\" height=\"200\" title=\"Layers in RBM\">\n",
    "\n",
    "Eq. 4\n",
    "Here is σ the Sigmoid function.\n",
    "\n",
    "*sampleHiddenLayer* takes the visible layer as input to calculate the hidden layer using Eq. 4 *h1Pdf* and then samples it to get * h1_sample*\n",
    "\n",
    "    v_sample: given visible layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    returns a sample vector of hidden layer and its distribution for a batch of data points\n",
    "    \n",
    "    hPdf: distribution of hidden layer; a matrix for batch of datapoints = p(h|v)\n",
    "    h_sample: sampled hidden layer matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def sampleHiddenLayer(v_sample):\n",
    "    \n",
    "    # write the code for calculation of hPdf using vectorized implementation of Eq 4\n",
    "    hPdf = sigmoid(np.dot(v_sample, W) + hiddenBias)# ~ 1 line\n",
    "    \n",
    "    # Here, np.random.binomial is used to create the hidden layer sample matrix\n",
    "    h_sample = np_rng.binomial(size=hPdf.shape, n=1, p=hPdf)\n",
    "    return [hPdf, h_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Visible Layer\n",
    "Similarly, the probability that a binary state of a visible neuron i is set to 1 is:\n",
    "<img src=\"figure7.png\" width=\"400\" height=\"200\" title=\"Layers in RBM\">\n",
    "Eq. 5\n",
    "\n",
    "As seen in equation 5, we will be writing a function to sample the Visible Layer.\n",
    "This function samples the visible layer based on the sampled data of hidden layer. <br>\n",
    "\n",
    "There are some differences in writing the function *sampleVisibleLayer*. <br>Firstly, we use np.random.multinomial to sample the visible layer *v_sample* from the distribution *vPdf*. <br>Secondly,elements of *vPdf* needs to sum to 1 as the function np.random.multinomial used to sample the visible layer takes on probability distributions as *pvals*. In other words, you are finding the softmax values. <br> Thirdly, we also make use of the *D* to sample the visible layer as each document has different word count.\n",
    "    \n",
    "    h_sample: given hidden layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    D: array of the sum of the row of the data vector; vector containing number of words in each document\n",
    "    \n",
    "    returns a sample vector of hidden layer and its distribution for a batch of data points\n",
    "    \n",
    "    vPdf: distribution of visible layer; a matrix for batch of datapoints = p(v|h)\n",
    "    v_sample: sampled visible layer matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def sampleVisibleLayer(h_sample, D):\n",
    "    \n",
    "    # complete the following function such that vPdf has the sum of entries equal to 1 for each of the datapoints in the batch\n",
    "    # you have to use axis = 1 in writing the denominator\n",
    "    numerator = np.exp(np.dot(h_sample, W.T) + visibleBias) # ~1 line\n",
    "    denominator = numerator.sum(axis=1).reshape((batchSize, 1))# ~1 line\n",
    "    vPdf = numerator/denominator  # ~1 line np.matmul\n",
    "   \n",
    "    # Here np.random.multinomial is used to sample as each document has different number of words \n",
    "    # and hence D is also a parameter in sampling\n",
    "    v_sample = np.zeros((batchSize, vPdf.shape[1]))\n",
    "    \n",
    "    for i in range(batchSize):\n",
    "        v_sample[i] = np_rng.multinomial(size=1, n=D[i], pvals=vPdf[i])\n",
    "        \n",
    "    return [vPdf, v_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above functions to write the function *gibbs* to run one iteration of gibbs sampling. Note that we are calculating the visible layer samples first and then using it to calculate he hidden layer sample. It'll become clear soon why we are doing so when you write the function for Contrastive Divergence.\n",
    "    \n",
    "    Input:\n",
    "    h_sample: given hidden layer matrix; matrix because a batch of data points will be trained at one go\n",
    "    D: array of the sum of the row of the data vector; vector containing number of words in each document\n",
    "    \n",
    "    Output:\n",
    "    vPdf: distribution of visible layer\n",
    "    v_sample: sampled visible layer matrix\n",
    "    hPdf: distribution of hidden layer\n",
    "    h_sample: sampled hidden layer matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def gibbs(h_sample, D):    \n",
    "    #use sampleVIsibleLayer and sampleHiddenLayer \n",
    "    vPdf, v_sample = sampleVisibleLayer(h_sample, D)# ~1 line\n",
    "    hPdf, h_sample = sampleHiddenLayer(v_sample)# ~1 line\n",
    "    return [vPdf, v_sample, hPdf, h_sample ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Contrastive Divergence\n",
    "\n",
    "You have learned that Contrastive Divergence updates weights after one iteration of Gibbs Sampling. Here, we shall perform *k* such iterations in Contrastive Divergence. \n",
    "The update of the weight matrix happens post the Contrastive Divergence step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we will writing a funtion to run the contrastive divergence algorithm in k steps\n",
    "    \n",
    "    Input:\n",
    "    data: batch data (visible layer)\n",
    "    k: no of iterations for gibbs sampling\n",
    "    \n",
    "    Output:\n",
    "    hiddenPDF_data: distribution of the hidden layer based on data\n",
    "    visibleSamples: visible samples generated by gibbs sampling \n",
    "    hiddenPDF: distribution of the hidden layer based on samples generated by gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def cd_k(data,k):\n",
    "    D = data.sum(axis=1)\n",
    "    hiddenPDF_data, hiddenSample_data = sampleHiddenLayer(data)# sample the hidden layer using the input data\n",
    "    chain_start = hiddenSample_data\n",
    "\n",
    "    for step in range(k):\n",
    "        if step == 0:\n",
    "            visiblePDF, visibleSamples, hiddenPDF, hiddenSamples  = gibbs(chain_start, D)# perform gibbs sampling using chain_start\n",
    "        else:\n",
    "            visiblePDF, visibleSamples, hiddenPDF, hiddenSamples = gibbs(hiddenSamples, D)# perform gibbs sampling using hiddenSamples\n",
    "    return hiddenPDF_data, visibleSamples, hiddenPDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Vectors $v_0$ and $v_k$ are used to calculate the activation probabilities for hidden layers $p(h_0|v_0)$ and $p(h_k|v_k)$ (Eq.4). The difference between the outer products of those probabilities with input vectors $v_0$ and $v_k$ results in the update matrix:\n",
    "<img src=\"figure9.png\" width=\"300\" height=\"200\" title=\"Layers in RBM\">\n",
    "Eq. 6. Update matrix. **Figure out** the vectorized implementation for this.\n",
    "\n",
    "In order to calculate $\\Delta (bias)$, <br>\n",
    "<center>$\\Delta (visiblebias) = average\\_ across\\_ batch(v_0 - v_k)$ </center> \n",
    "<center>$\\Delta (hiddenbias) = average\\_across\\_ batch(p(h_0|v_0) - p(h_k|v_k))$ </center> \n",
    "\n",
    "Using the update matrix the new weights can be calculated with momentum gradient ascent, given by:\n",
    "<center>  $mW_t = \\gamma \\ mW_{t-1} - \\Delta W$</center> \n",
    "<center>  $mvisiblebias_t = \\gamma \\ mvisiblebias_{t-1} - \\Delta visiblebias$</center>\n",
    "<center>  $mhiddenbias_t = \\gamma \\ mhiddenbias_{t-1} - \\Delta hiddenbias$</center><br>\n",
    "<center>  $W_t = W_{t-1} + \\alpha \\ mW_t$</center> \n",
    "<center>  $visiblebias_t = visiblebias_{t-1} + \\alpha \\ mvisiblebias_t$</center> \n",
    "<center>  $hiddenbias_t = hiddenbias_{t-1} + \\alpha \\ mhiddenbias_t$</center> \n",
    "\n",
    "\n",
    "Eq. 7. Update rule for the weights.\n",
    "\n",
    "Note that in the code implementation below <br>\n",
    " hiddenPDF_data is $p(h_0|v_0)$ <br>\n",
    " visibleSamples is $v_k$ <br>\n",
    " hiddenPDF is $p(h_k|v_k)$ <br>\n",
    " mdata is $v_0$ <br>\n",
    " eta is $\\alpha$ <br>\n",
    " mrate is $\\gamma$ <br>\n",
    "These will be helpful in writing the weight updates.\n",
    "\n",
    "In this we will write a function which iterates over our data for *epochs*.\n",
    "At every epoch we shuffle the data and then run CD on a mini batch size defined by *batchSize*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "visibleUnits: no of words in your Bag of words Model\n",
    "hiddenUnits: no of topics\n",
    "batchSize: data slice to be selected \n",
    "epochs: no of iterations\n",
    "eta: learning rate\n",
    "mrate: momentum coefficient\n",
    "W : weights between the visible and hidden layer\n",
    "visibleBias, hiddenBias: biases for visible and hidden layer respectively\n",
    "\"\"\"\n",
    "\n",
    "# define batch size\n",
    "batchSize = 200\n",
    "\n",
    "epochs = 100\n",
    "eta = 0.05 \n",
    "mrate = 0.5\n",
    "\n",
    "# initialise weights\n",
    "weightinit=0.01\n",
    "W = weightinit * np_rng.randn(visibleUnits, hiddenUnits)\n",
    "visibleBias = weightinit * np_rng.randn(visibleUnits)\n",
    "hiddenBias = np.zeros((hiddenUnits))\n",
    "\n",
    "# we shall use 10 interations of gibbs sampling\n",
    "k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graded\n",
    "def train(dataX,k):\n",
    "    global W,visibleBias,hiddenBias,mrate,batchSize,epochs # calling the variables initiliazed at the begining\n",
    "    \n",
    "    mW = np.asarray( np_rng.uniform(\n",
    "                    low=-1 * np.sqrt(2. / (hiddenUnits + visibleUnits)),\n",
    "                    high=1 * np.sqrt(2. / (hiddenUnits + visibleUnits)),\n",
    "                    size=(visibleUnits, hiddenUnits)\n",
    "                ),dtype=np.float64)# initialise momentum_weights\n",
    "    mvisibleBias =  np.zeros(visibleUnits)# initialise momentum_visiblebiases\n",
    "    mhiddenBias =  np.zeros(hiddenUnits)# initialise momentum_hiddenbiases\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epochs #\" , epoch)\n",
    "        np_rng.shuffle(dataX) #shuffling the data\n",
    "        \n",
    "        for i in range(0, dataX.shape[0], batchSize):\n",
    "            mData = dataX[i:i+batchSize]#select a batch of datapoints\n",
    "                        \n",
    "            hiddenPDF_data, visibleSamples, hiddenPDF =  cd_k(mData,k)# perfrom Contrastive Divergence on the batch for k iterations\n",
    "            \n",
    "            mW = mrate*mW - (np.dot(mData.T,hiddenPDF_data) - np.dot(mData.T,hiddenPDF))#write the momentum update equation for weight matrix\n",
    "            \n",
    "            mvisibleBias =  mrate*mvisibleBias - np.mean(mData-visibleSamples, axis=0)#write the momentum update equation for visiblebias vector\n",
    "            mhiddenBias =  mrate*mhiddenBias - np.mean(hiddenPDF_data - hiddenPDF, axis=0) #write the momentum update equation for hiddenbias vector\n",
    "\n",
    "            W = W + eta*mW #weight update equation\n",
    "            visibleBias = visibleBias + eta*mvisibleBias #visible bias update equation\n",
    "            hiddenBias = hiddenBias + eta*mhiddenBias #hidden bias update equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This will take around 10 minutes of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs # 0\n",
      "epochs # 1\n",
      "epochs # 2\n",
      "epochs # 3\n",
      "epochs # 4\n",
      "epochs # 5\n",
      "epochs # 6\n",
      "epochs # 7\n",
      "epochs # 8\n",
      "epochs # 9\n",
      "epochs # 10\n",
      "epochs # 11\n",
      "epochs # 12\n",
      "epochs # 13\n",
      "epochs # 14\n",
      "epochs # 15\n",
      "epochs # 16\n",
      "epochs # 17\n",
      "epochs # 18\n",
      "epochs # 19\n",
      "epochs # 20\n",
      "epochs # 21\n",
      "epochs # 22\n",
      "epochs # 23\n",
      "epochs # 24\n",
      "epochs # 25\n",
      "epochs # 26\n",
      "epochs # 27\n",
      "epochs # 28\n",
      "epochs # 29\n",
      "epochs # 30\n",
      "epochs # 31\n",
      "epochs # 32\n",
      "epochs # 33\n",
      "epochs # 34\n",
      "epochs # 35\n",
      "epochs # 36\n",
      "epochs # 37\n",
      "epochs # 38\n",
      "epochs # 39\n",
      "epochs # 40\n",
      "epochs # 41\n",
      "epochs # 42\n",
      "epochs # 43\n",
      "epochs # 44\n",
      "epochs # 45\n",
      "epochs # 46\n",
      "epochs # 47\n",
      "epochs # 48\n",
      "epochs # 49\n",
      "epochs # 50\n",
      "epochs # 51\n",
      "epochs # 52\n",
      "epochs # 53\n",
      "epochs # 54\n",
      "epochs # 55\n",
      "epochs # 56\n",
      "epochs # 57\n",
      "epochs # 58\n",
      "epochs # 59\n",
      "epochs # 60\n",
      "epochs # 61\n",
      "epochs # 62\n",
      "epochs # 63\n",
      "epochs # 64\n",
      "epochs # 65\n",
      "epochs # 66\n",
      "epochs # 67\n",
      "epochs # 68\n",
      "epochs # 69\n",
      "epochs # 70\n",
      "epochs # 71\n",
      "epochs # 72\n",
      "epochs # 73\n",
      "epochs # 74\n",
      "epochs # 75\n",
      "epochs # 76\n",
      "epochs # 77\n",
      "epochs # 78\n",
      "epochs # 79\n",
      "epochs # 80\n",
      "epochs # 81\n",
      "epochs # 82\n",
      "epochs # 83\n",
      "epochs # 84\n",
      "epochs # 85\n",
      "epochs # 86\n",
      "epochs # 87\n",
      "epochs # 88\n",
      "epochs # 89\n",
      "epochs # 90\n",
      "epochs # 91\n",
      "epochs # 92\n",
      "epochs # 93\n",
      "epochs # 94\n",
      "epochs # 95\n",
      "epochs # 96\n",
      "epochs # 97\n",
      "epochs # 98\n",
      "epochs # 99\n"
     ]
    }
   ],
   "source": [
    "train(trainX.toarray(),k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Distribution based on Topics\n",
    "In this function, we are finding the distribution of words over the topics. You can take a look at the words under each topic and see what they are talking about. The number of topics is the number of neurons in the hidden layer. <br>\n",
    "<br>\n",
    "\n",
    "For each topic, the function prints the top 15 words that describe the topic. You can see that some of the words occur in multiple topics.\n",
    "\n",
    "    topic: number of hidden layers\n",
    "    voc: indexing of the vocabulary\n",
    "    \n",
    "Feel free to change the number of iterations of gibbs sampling in Contrastive Divergence and see how the distribution of words change under the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = defaultdict(list)\n",
    "def worddist( topic, voc):\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize every topic =1 once \n",
    "    \"\"\"\n",
    "    global topic_words\n",
    "    vecTopics = np.zeros((topic, topic))\n",
    "    for i in range(len(vecTopics)):\n",
    "        vecTopics[i][i] = 1\n",
    "    \n",
    "    for i, vecTopic in enumerate(vecTopics):\n",
    "       \n",
    "        numerator = np.exp(np.dot(vecTopic, W.T) + visibleBias)\n",
    "        denominator = numerator.sum().reshape((1, 1))\n",
    "        word_distribution = (numerator/denominator).flatten()\n",
    "        \n",
    "        tmpDict = {}\n",
    "        for j in voc.keys():\n",
    "            tmpDict[j] = word_distribution[voc[j]]\n",
    "        print('topic', str(i), ':', vecTopic)\n",
    "        lm=0\n",
    "        for word, prob in sorted(tmpDict.items(), key=lambda x:x[1], reverse=True):\n",
    "            print ( word, str(prob))\n",
    "            topic_words[i].append(word)\n",
    "            lm+=1\n",
    "            if lm==15:\n",
    "                break\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : [1. 0. 0. 0. 0.]\n",
      "procedure 0.000634285977175353\n",
      "specially 0.0006340428409141284\n",
      "casing 0.0006336596674772878\n",
      "course 0.0006331009132529704\n",
      "350 0.00063255953436549\n",
      "build 0.0006319170288527369\n",
      "messaging 0.0006313938582913948\n",
      "disapointing 0.0006307029366793908\n",
      "aspect 0.0006296386442826928\n",
      "christmas 0.0006294881207015874\n",
      "bluetooths 0.0006293073547480014\n",
      "top 0.0006291686031157599\n",
      "funny 0.0006286690106485316\n",
      "thanks 0.0006286143885703104\n",
      "row 0.000627704406980423\n",
      "\n",
      "\n",
      "topic 1 : [0. 1. 0. 0. 0.]\n",
      "procedure 0.000642465568151984\n",
      "windows 0.000639532678133595\n",
      "drains 0.0006362136895341591\n",
      "funny 0.0006332184449383049\n",
      "disapointing 0.0006326540249438052\n",
      "moving 0.0006322968963544224\n",
      "source 0.0006319827556500726\n",
      "shifting 0.0006310035212463083\n",
      "fully 0.0006308711074326133\n",
      "study 0.0006306912375209815\n",
      "row 0.0006303861382089941\n",
      "followed 0.0006285116121270782\n",
      "specially 0.0006283258726084885\n",
      "exceeds 0.00062798259432602\n",
      "rubber 0.0006277769231506801\n",
      "\n",
      "\n",
      "topic 2 : [0. 0. 1. 0. 0.]\n",
      "casing 0.0006366927137877419\n",
      "crack 0.0006347671387015114\n",
      "windows 0.0006347227383346686\n",
      "drains 0.0006329680710000609\n",
      "transfer 0.0006326959416413813\n",
      "jabra350 0.0006312560419784966\n",
      "certain 0.0006306512099734221\n",
      "rubber 0.0006295011389231676\n",
      "fully 0.0006293427658123942\n",
      "note 0.0006290199162026804\n",
      "talking 0.0006285064481231364\n",
      "shows 0.0006284485328822221\n",
      "refurb 0.0006281162115155\n",
      "disapointing 0.0006277709245098506\n",
      "moving 0.0006272732255451466\n",
      "\n",
      "\n",
      "topic 3 : [0. 0. 0. 1. 0.]\n",
      "procedure 0.0006358264949969889\n",
      "bluetooths 0.0006352590218295838\n",
      "sitting 0.0006326170220068625\n",
      "address 0.000632346362887619\n",
      "messaging 0.0006316757401936721\n",
      "specially 0.0006303321465865341\n",
      "build 0.0006300061238355339\n",
      "imagine 0.0006288781742887551\n",
      "regretted 0.0006288672319277357\n",
      "crack 0.0006287111366973352\n",
      "350 0.00062813674484937\n",
      "disapointing 0.0006280140825267472\n",
      "note 0.0006274124333730972\n",
      "lap 0.0006272416074773537\n",
      "drains 0.0006269369428350301\n",
      "\n",
      "\n",
      "topic 4 : [0. 0. 0. 0. 1.]\n",
      "sitting 0.0006368354384901277\n",
      "discomfort 0.0006341297831525541\n",
      "removing 0.0006339043789783861\n",
      "overly 0.0006324284503098213\n",
      "wonderfully 0.0006318301193809459\n",
      "allot 0.0006316263860348928\n",
      "rubber 0.0006312072368784293\n",
      "procedure 0.0006307957298130976\n",
      "messaging 0.0006291547552164297\n",
      "submerged 0.000628633200077034\n",
      "funny 0.0006286307361819114\n",
      "transfer 0.0006286082708084401\n",
      "riingtones 0.0006278420636430614\n",
      "shifting 0.0006277512232568351\n",
      "disapointing 0.0006276952933282919\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worddist( hiddenUnits, tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the probability assigned to each word for ever topic present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "topic #0\n",
      "Best I've found so far .... I've tried 2 other bluetooths and this one has the best quality (for both me and the listener) as well as ease of using.\n",
      "I love my 350 headset.. My Jabra350 bluetooth headset is great, the reception is very good and the ear piece is a comfortable fit.\n",
      "I know that sounds funny, but to me it seemed like sketchy technology that wouldn't work well.Well, this one works great.\n",
      "This phone is very fast with sending any kind of messages and web browsing is significantly faster than previous phones i have used.\n",
      "Nice quality build, unlike some cheap s*** out there.\n",
      "The file browser offers all the options that one needs.Handsfree is great.\n",
      "It is very comfortable to wear as well, which is probably the most important aspect about using a case.\n",
      "Also, if your phone is dropped, this case is not going to save it, specially when dropped face down.\n",
      "I especially love the long battery life.\n",
      "Another note about this phone's appearance is that it really looks rather bland, especially in the all black model.\n",
      "My Sanyo has survived dozens of drops on blacktop without ill effect.\n",
      "The update procedure is difficult and cumbersome.\n",
      "It is super charged up for use as a small hybrid palmtop/camera/cellphone, and excels in those roles.\n",
      "I own a Jabra Earset and was very happy with it, but the sound quality, especially outgoing, on this is better.\n",
      "Muddy, low quality sound, and the casing around the wire's insert was poorly super glued and slid off.\n",
      "Due to this happening on every call I was forced to stop using this headset.\n",
      "It works great with a car charger, especially if you cannot plug in two adapters at the same time.\n",
      "Verizon tech support walked my through a few procedures, none of which worked and I ended up having to do a hard re-set, wiping out all my data.\n",
      "The real killer is the volume, and of course it breaking.\n",
      "the charger worked for about a week and then completely stopped charging my phone.\n",
      "The calls drop, the phone comes on and off at will, the screen goes black and the worst of all it stops ringing intermittently.\n",
      "Reaching for the bottom row is uncomfortable, and the send and end keys are not where I expect them to be.3.\n",
      "This is an excellent tool, especially when paired with your phone's auto-answer.\n",
      "But, in any case, the best part is, you can download these pictures to your laptop using IR, or even send pictures from your laptop to the phone.\n",
      "I didn't want the clip going over the top of my ear, causing discomfort.\n",
      "However, after about a year, the fliptop started to get loose and wobbly and eventually my screen went black and I couldn't receive and place calls.\n",
      "The text messaging feature is really tricky to use.\n",
      "\n",
      "\n",
      "topic #1\n",
      "I went on Motorola's website and followed all directions, but could not get it to pair again.\n",
      "It is unusable in a moving car at freeway speed.\n",
      "I know that sounds funny, but to me it seemed like sketchy technology that wouldn't work well.Well, this one works great.\n",
      "This phone is very fast with sending any kind of messages and web browsing is significantly faster than previous phones i have used.\n",
      "The file browser offers all the options that one needs.Handsfree is great.\n",
      "This product had a strong rubber/petroleum smell that was unbearable after a while and caused me to return it\n",
      "Also its slim enough to fit into my alarm clock docking station without removing the case.\n",
      "Also, if your phone is dropped, this case is not going to save it, specially when dropped face down.\n",
      "I especially love the long battery life.\n",
      "Another note about this phone's appearance is that it really looks rather bland, especially in the all black model.\n",
      "I even fully charged it before I went to bed and turned off blue tooth and wi-fi and noticed that it only had 20 % left in the morning.\n",
      "The update procedure is difficult and cumbersome.\n",
      "I received my orders well within the shipping timeframe, everything was in good working order and overall, I am very excited to have this source.\n",
      "No shifting, no bubbling, no peeling, not even a scratch, NOTHING!I couldn't be more happier with my new one for the Droid.\n",
      "I own a Jabra Earset and was very happy with it, but the sound quality, especially outgoing, on this is better.\n",
      "It works great with a car charger, especially if you cannot plug in two adapters at the same time.\n",
      "Verizon tech support walked my through a few procedures, none of which worked and I ended up having to do a hard re-set, wiping out all my data.\n",
      "A must study for anyone interested in the \"worst sins\" of industrial design.\n",
      "The biggest complaint I have is, the battery drains superfast.\n",
      "Unfortunately it will not recharge my iPhone 4s, despite connecting it from multiple power sources (iMac, external battery, wall outlet, etc).\n",
      "I have a Verizon LG phone and they work well together, good reception and range that exceeds 20 feet line of sight.\n",
      "Reaching for the bottom row is uncomfortable, and the send and end keys are not where I expect them to be.3.\n",
      "This is an excellent tool, especially when paired with your phone's auto-answer.\n",
      "Leopard Print is wonderfully wild!.\n",
      "\n",
      "\n",
      "topic #2\n",
      "It is unusable in a moving car at freeway speed.\n",
      "I also didn't like the \"on\" button, it felt like it would crack with use.\n",
      "These are certainly very comfortable and functionality is decent.\n",
      "This product had a strong rubber/petroleum smell that was unbearable after a while and caused me to return it\n",
      "Everything worked on the first try.The device was certainly engineered in a clever way and the construction feels good.\n",
      "Also its slim enough to fit into my alarm clock docking station without removing the case.\n",
      "Another note about this phone's appearance is that it really looks rather bland, especially in the all black model.\n",
      "I even fully charged it before I went to bed and turned off blue tooth and wi-fi and noticed that it only had 20 % left in the morning.\n",
      "As an earlier review noted, plug in this charger and nothing happens.\n",
      "All it took was one drop from about 6 inches above the kitchen counter and it was cracked.I am not impressed and I am not laughing.\n",
      "Muddy, low quality sound, and the casing around the wire's insert was poorly super glued and slid off.\n",
      "I used bitpim (a free program you can find on the internet)to transfer data to the phone.The price of the cable was excellent.\n",
      "Linksys should have some way to exchange a bad phone for a refurb unit or something!\n",
      "The biggest complaint I have is, the battery drains superfast.\n",
      "This case has passed the one year mark and while it shows signs of wear, it is 100% functional.\n",
      "I tried talking real loud but shouting on the telephone gets old and I was still told it wasn't great.\n",
      "Leopard Print is wonderfully wild!.\n",
      "I cannot make calls at certain places.\n",
      "\n",
      "\n",
      "topic #3\n",
      "If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\n",
      "Best I've found so far .... I've tried 2 other bluetooths and this one has the best quality (for both me and the listener) as well as ease of using.\n",
      "I love my 350 headset.. My Jabra350 bluetooth headset is great, the reception is very good and the ear piece is a comfortable fit.\n",
      "Nice quality build, unlike some cheap s*** out there.\n",
      "I also didn't like the \"on\" button, it felt like it would crack with use.\n",
      "I was sitting in my vehicle, with the cradle on my belt, and the headset lost signal.\n",
      "Also, if your phone is dropped, this case is not going to save it, specially when dropped face down.\n",
      "I especially love the long battery life.\n",
      "Another note about this phone's appearance is that it really looks rather bland, especially in the all black model.\n",
      "The update procedure is difficult and cumbersome.\n",
      "As an earlier review noted, plug in this charger and nothing happens.\n",
      "All it took was one drop from about 6 inches above the kitchen counter and it was cracked.I am not impressed and I am not laughing.\n",
      "I own a Jabra Earset and was very happy with it, but the sound quality, especially outgoing, on this is better.\n",
      "It works great with a car charger, especially if you cannot plug in two adapters at the same time.\n",
      "Verizon tech support walked my through a few procedures, none of which worked and I ended up having to do a hard re-set, wiping out all my data.\n",
      "The handsfree part works fine, but then the car tries to download the address book, and the Treo reboots.Overall, I still rate this device high.\n",
      "The biggest complaint I have is, the battery drains superfast.\n",
      "I bought this phone as a replacement for my StarTac and have regretted it since.\n",
      "This is an excellent tool, especially when paired with your phone's auto-answer.\n",
      "But, in any case, the best part is, you can download these pictures to your laptop using IR, or even send pictures from your laptop to the phone.\n",
      "I does not maintain a connection with the computer while it is on my lap.\n",
      "The text messaging feature is really tricky to use.\n",
      "\n",
      "\n",
      "topic #4\n",
      "I even dropped this phone into a stream and it was submerged for 15 seconds and it still works great!\n",
      "I know that sounds funny, but to me it seemed like sketchy technology that wouldn't work well.Well, this one works great.\n",
      "This product had a strong rubber/petroleum smell that was unbearable after a while and caused me to return it\n",
      "Also its slim enough to fit into my alarm clock docking station without removing the case.\n",
      "However-the riingtones are not the best, and neither are the games.\n",
      "I was sitting in my vehicle, with the cradle on my belt, and the headset lost signal.\n",
      "The update procedure is difficult and cumbersome.\n",
      "No shifting, no bubbling, no peeling, not even a scratch, NOTHING!I couldn't be more happier with my new one for the Droid.\n",
      "Verizon tech support walked my through a few procedures, none of which worked and I ended up having to do a hard re-set, wiping out all my data.\n",
      "I used bitpim (a free program you can find on the internet)to transfer data to the phone.The price of the cable was excellent.\n",
      "They do not last forever, but is not overly expensive to replace.Easy to operate and the sound is much better than others I have tried.\n",
      "I didn't want the clip going over the top of my ear, causing discomfort.\n",
      "Leopard Print is wonderfully wild!.\n",
      "I really like this product over the Motorola because it is allot clearer on the ear piece and the mic.\n",
      "The text messaging feature is really tricky to use.\n"
     ]
    }
   ],
   "source": [
    "topics_reviews = defaultdict(list)\n",
    "#topic_words\n",
    "def get_topic(topic):\n",
    "    return list([df[\"Text\"][i] for i in range(len(df['Text'])) if any(ele in df[\"Text\"][i] for ele in topic_words[topic])])\n",
    "topics_reviews = {topic: get_topic(topic)  for topic in range(0,5)}\n",
    "\n",
    "for i in topics_reviews:\n",
    "    print(\"\\n\\ntopic #\"+str(i))\n",
    "    for text in topics_reviews[i]:\n",
    "        print(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
